includes:
  - configs/datasets/okvqa/bert.yaml


# TODO: how much is necessary here?

model_config:
  vqtor:
    losses:
      #- type: cross_entropy, target dim needs to be adjusted
      - type: logit_bce # binary cross-entropy loss since each question has multiple answers (MGFAN with ref.)

dataset_config:
  okvqa:
    processors:
      answer_processor:
        params:
          # improved answer vocab
          vocab_file: okvqa/defaults/annotations/annotations/answer_vocab_count10.txt
    dump_output_dir: ${env.save_dir}
    dump_pred_info: false



scheduler:
  #type: warmup_linear
  type: warmup_cosine # varying
  params:
    num_warmup_steps: 2000 # default: 2000
    num_training_steps: ${training.max_updates}

# similare to pythia, (Adamax better at embeddings)
optimizer:
  type: Adamax
  params:
    eps: 1.0e-08
    lr: 0.01
    weight_decay: 0

evaluation:
  metrics:
    - accuracy
    - vqa_accuracy
    #- binary_f1
    #- roc_auc

training:
  batch_size: 128 # default: 512, 128 or higher performs better (2017 tips and tricks)
  #max_epochs: 1 # no default specified, 12-18 mentioned by (2017 tips and tricks)
  lr_scheduler: true   # Don't forget to update schedule_attributes if you update this
  max_updates: 66000 # default: 22000, krisp uses 88000
  num_workers: 4 # default is 4, rule of thumb, set to num cpu cores
  tensorboard: true
  use_warmup: true
  evaluate_metrics: true # enable evaluation every log_interval number of updates
  evaluation_interval: 1000
  find_unused_parameters: true # Turn on if you want to ignore unused parameters in case of DDP
  early_stop:
    criteria: okvqa/vqa_accuracy
    minimize: false

trainer:
  params:
    gpus: 1 # TODO: what works best
    logger: true
    progress_bar_refresh_rate: 0.1
    val_check_interval: 5000 # checkpointing

