
includes:
    # include running experiment configs (defaults for all)
  - configs/experiments/defaults.yaml
    # define image encoder or keep resnet50 as default
  - configs/experiments/image_encoder/grids.yaml
    # importing graph module
  - configs/experiments/graph/krisp.yaml
    # default fusion is simple concat
  - configs/experiments/fusion/multiply.yaml
    # default classifier is mlp
  - configs/experiments/classifier/logistic.yaml
# overwrite included if necessary:

env:
  save_dir: ${env.part_save_dir}/ques_graph_guided


model_config:
  qlarifais:

    attention:
      use: true
      type: question_graph_guided
      params:
        # used as activation in combine layer in attention
        graph_dim: ${model_config.qlarifais.graph_hidden_size}
        modal_combine: # combining img and text
          type: non_linear_element_multiply # [-multiply, -concat]
          params:
            dropout: 0 # avoid overfitting
            hidden_dim: ${model_config.qlarifais.attention_hidden_dim} # or set to 2048?
        transform:
          type: linear
          params:
            out_dim: 1 # attention per feature
        normalization: softmax




