# include running experiment configs
includes:
  - configs/experiments/defaults.yaml
    # default image encoder is resnet50

# overwrite if necessary

env:
  save_dir: ${env.part_save_dir}/grids


dataset_config:
  okvqa:
    # only load the images and not pre-extracted features
    depth_first: false
    fast_read: true
    use_images: true
    use_features: false
    zoo_requirements:
    - okvqa.defaults
    images:
      train:
      - okvqa/defaults/images/
      val:
      - okvqa/defaults/images/
      test:
      - okvqa/defaults/images/

model_config:
  qlarifais:
    # models from grid-feats-vqa: https://github.com/facebookresearch/grid-feats-vqa
    image_encoder:
      type: grid_feats_vqa
      params:
        name: grid_feats_vqa
        # specify backbone and type of features
        # choices: [R-50-(grid or updn), X-101-grid, X-152-(grid or challenge(MoVie+GridFeat))]
        model: R-50-grid.yaml
        output_dir: ${env.save_dir}
        pretrained: true
        pool_type: avg
        zero_init_residual: false
        num_output_features: 1
      # how to resize features
      resize: average_pooling
      num_features: 49 # based on okvqa image dim, 224x224 and grid feats 32x32 sized stride (i.e. 224^2/32^2 = 7*7
