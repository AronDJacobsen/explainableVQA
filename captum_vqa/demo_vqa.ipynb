{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e614ef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7767cb1",
   "metadata": {},
   "source": [
    "### https://captum.ai/tutorials/Multimodal_VQA_Captum_Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ab3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "PYTORCH_VQA_DIR = os.path.realpath(f\"{os.getcwd()}{os.sep}pytorch_vqa\")\n",
    "PYTORCH_RESNET_DIR = os.path.realpath(f\"{os.getcwd()}{os.sep}pytorch_resnet\")\n",
    "\n",
    "# Please modify this path to where it is located on your machine\n",
    "# you can download this model from: \n",
    "# https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth\n",
    "VQA_MODEL_PATH = \"models/2017-08-04_00.55.19.pth\"\n",
    "\n",
    "assert(os.path.exists(PYTORCH_VQA_DIR))\n",
    "assert(os.path.exists(PYTORCH_RESNET_DIR))\n",
    "assert(os.path.exists(VQA_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52c823c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from pytorch_resnet import resnet  # from pytorch-resnet\n",
    "except:\n",
    "    print(\"please provide a valid path to pytorch-resnet\")\n",
    "\n",
    "try:\n",
    "    from pytorch_vqa.model import Net, apply_attention, tile_2d_over_nd  # from pytorch-vqa\n",
    "    from pytorch_vqa.utils import get_transform  # from pytorch-vqa\n",
    "except:\n",
    "    print(\"please provide a valid path to pytorch-vqa\")\n",
    "    \n",
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature, TextFeature\n",
    "from captum.attr import TokenReferenceBase, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38caab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the device we will use for model inference\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9daa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_state = torch.load(VQA_MODEL_PATH, map_location=device)\n",
    "\n",
    "# reading vocabulary from saved model\n",
    "vocab = saved_state[\"vocab\"]\n",
    "\n",
    "# reading word tokens from saved model\n",
    "token_to_index = vocab[\"question\"]\n",
    "\n",
    "# reading answers from saved model\n",
    "answer_to_index = vocab[\"answer\"]\n",
    "\n",
    "num_tokens = len(token_to_index) + 1\n",
    "\n",
    "# reading answer classes from the vocabulary\n",
    "answer_words = [\"unk\"] * len(answer_to_index)\n",
    "for w, idx in answer_to_index.items():\n",
    "    answer_words[idx] = w\n",
    "\n",
    "vqa_net = torch.nn.DataParallel(Net(num_tokens), device_ids=[0, 1])\n",
    "vqa_net.load_state_dict(saved_state[\"weights\"])\n",
    "vqa_net = vqa_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f140a624",
   "metadata": {},
   "outputs": [],
   "source": [
    " # for visualization to convert indices to tokens for questions\n",
    "question_words = [\"unk\"] * num_tokens\n",
    "for w, idx in token_to_index.items():\n",
    "    question_words[idx] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01a80232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r_model = resnet.resnet152(pretrained=True)\n",
    "        self.r_model.eval()\n",
    "        self.r_model.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.r_model.conv1(x)\n",
    "        x = self.r_model.bn1(x)\n",
    "        x = self.r_model.relu(x)\n",
    "        x = self.r_model.maxpool(x)\n",
    "        x = self.r_model.layer1(x)\n",
    "        x = self.r_model.layer2(x)\n",
    "        x = self.r_model.layer3(x)\n",
    "        return self.r_model.layer4(x)\n",
    "\n",
    "class VQA_Resnet_Model(Net):\n",
    "    def __init__(self, embedding_tokens):\n",
    "        super().__init__(embedding_tokens)\n",
    "        self.resnet_layer4 = ResNetLayer4()\n",
    "\n",
    "    def forward(self, v, q, q_len):\n",
    "        q = self.text(q, list(q_len.data))\n",
    "        v = self.resnet_layer4(v)\n",
    "\n",
    "        v = v / (v.norm(p=2, dim=1, keepdim=True).expand_as(v) + 1e-8)\n",
    "\n",
    "        a = self.attention(v, q)\n",
    "        v = apply_attention(v, a)\n",
    "\n",
    "        combined = torch.cat([v, q], dim=1)\n",
    "        answer = self.classifier(combined)\n",
    "        return answer\n",
    "\n",
    "vqa_resnet = VQA_Resnet_Model(vqa_net.module.text.embedding.num_embeddings)\n",
    "\n",
    "# `device_ids` contains a list of GPU ids which are used for parallelization supported by `DataParallel`\n",
    "vqa_resnet = torch.nn.DataParallel(vqa_resnet, device_ids=[0, 1])\n",
    "\n",
    "# saved vqa model's parameters\n",
    "partial_dict = vqa_net.state_dict()\n",
    "\n",
    "state = vqa_resnet.state_dict()\n",
    "state.update(partial_dict)\n",
    "vqa_resnet.load_state_dict(state)\n",
    "\n",
    "vqa_resnet.to(device)\n",
    "vqa_resnet.eval()\n",
    "\n",
    "# This is original VQA model without resnet. Removing it, since we do not need it\n",
    "del vqa_net\n",
    "\n",
    "# this is necessary for the backpropagation of RNNs models in eval mode\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a537b456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytorch_resnet.resnet.ResNet"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resnet.resnet152(pretrained=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c25cbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 448  # scale image to given size and center\n",
    "central_fraction = 1.0\n",
    "\n",
    "transform = get_transform(image_size, central_fraction=central_fraction)\n",
    "transform_normalize = transform.transforms.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01da0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_question(question):\n",
    "    \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n",
    "    question = re.sub('[^A-Za-z0-9]+', ' ', question) # added, to remove special characters\n",
    "    question_arr = question.lower().split()\n",
    "    print(question_arr)\n",
    "    vec = torch.zeros(len(question_arr), device=device).long()\n",
    "    for i, token in enumerate(question_arr):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, torch.tensor(len(question_arr), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf42bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'up', 'do', 'you', 'like', 'the', 'weather']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([  3, 109,  28,  29, 102,   1, 207]), tensor(7))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_question(\"what up, do you like the weather?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d3216d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_image(x):\n",
    "    return x * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d277e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s194253\\.conda\\envs\\test_vqa\\lib\\site-packages\\captum\\attr\\_models\\base.py:188: UserWarning: In order to make embedding layers more interpretable they will be replaced with an interpretable embedding layer which wraps the original embedding layer and takes word embedding vectors as inputs of the forward function. This allows us to generate baselines for word embeddings and compute attributions for each embedding dimension. The original embedding layer must be set back by calling `remove_interpretable_embedding_layer` function after model interpretation is finished. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(\n",
    "    vqa_resnet, \"module.text.embedding\"\n",
    ")\n",
    "\n",
    "PAD_IND = token_to_index[\"pad\"]\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
    "\n",
    "def baseline_text(x):\n",
    "    seq_len = x.size(0)\n",
    "    ref_indices = token_reference.generate_reference(seq_len, device=device).unsqueeze(\n",
    "        0\n",
    "    )\n",
    "    return interpretable_embedding.indices_to_embeddings(ref_indices).squeeze(0)\n",
    "\n",
    "def input_text_transform(x):\n",
    "    return interpretable_embedding.indices_to_embeddings(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f7eb4",
   "metadata": {},
   "source": [
    "## Using the Insights API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf920830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_dataset(image, questions, targets):\n",
    "    img = Image.open(image).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    for question, target in zip(questions, targets):\n",
    "        q, q_len = encode_question(question)\n",
    "\n",
    "        q = q.unsqueeze(0)\n",
    "        q_len = q_len.unsqueeze(0)\n",
    "\n",
    "        target_idx = answer_to_index[target]\n",
    "\n",
    "        yield Batch(\n",
    "            inputs=(img, q), labels=(target_idx,), additional_args=q_len\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfcf392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(o):\n",
    "    return F.softmax(o, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4344ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def itos(input):\n",
    "    return [question_words[int(i)] for i in input.squeeze(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0399e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = vqa_dataset(\"./img/vqa/elephant.jpg\", \n",
    "    [\"what is on the picture\",\n",
    "    \"what color is the elephant\",\n",
    "    \"where is the elephant\" ],\n",
    "    [\"elephant\", \"gray\", \"safari\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee4339",
   "metadata": {},
   "source": [
    "# TODO: try to replace the featurizers with e.g. FRCNN and sen2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d131937c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object vqa_dataset at 0x0000014431FCB2E0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\n",
    "    ImageFeature(\n",
    "        \"Picture\",\n",
    "        input_transforms=[transform_normalize],\n",
    "        baseline_transforms=[baseline_image],\n",
    "    ),\n",
    "    TextFeature(\n",
    "        \"Question\",\n",
    "        input_transforms=[input_text_transform],\n",
    "        baseline_transforms=[baseline_text],\n",
    "        visualization_transform=itos,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4933d5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5465786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0921615",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sent2vec' has no attribute 'Sent2vecModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-91e62ddcfb5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSent2vecModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.bin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"once upon a time .\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sent2vec' has no attribute 'Sent2vecModel'"
     ]
    }
   ],
   "source": [
    "import sent2vec\n",
    "\n",
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('model.bin')\n",
    "emb = model.embed_sentence(\"once upon a time .\") \n",
    "embs = model.embed_sentences([\"first sentence .\", \"another sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "785f7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sent2vec import vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae1995db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sent2vec' from 'C:\\\\Users\\\\s194253\\\\.conda\\\\envs\\\\test_vqa\\\\lib\\\\site-packages\\\\sent2vec\\\\__init__.py'>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f8bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQA environment",
   "language": "python",
   "name": "test_vqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
