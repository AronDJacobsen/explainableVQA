{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7767cb1",
   "metadata": {},
   "source": [
    "### https://captum.ai/tutorials/Multimodal_VQA_Captum_Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27ab3236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "PYTORCH_VQA_DIR = os.path.realpath(f\"{os.getcwd()}{os.sep}pytorch_vqa\")\n",
    "PYTORCH_RESNET_DIR = os.path.realpath(f\"{os.getcwd()}{os.sep}pytorch_resnet\")\n",
    "\n",
    "# Please modify this path to where it is located on your machine\n",
    "# you can download this model from: \n",
    "# https://github.com/Cyanogenoid/pytorch-vqa/releases/download/v1.0/2017-08-04_00.55.19.pth\n",
    "VQA_MODEL_PATH = \"models/2017-08-04_00.55.19.pth\"\n",
    "\n",
    "assert(os.path.exists(PYTORCH_VQA_DIR))\n",
    "assert(os.path.exists(PYTORCH_RESNET_DIR))\n",
    "assert(os.path.exists(VQA_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c823c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    from pytorch_resnet import resnet  # from pytorch-resnet\n",
    "except:\n",
    "    print(\"please provide a valid path to pytorch-resnet\")\n",
    "\n",
    "try:\n",
    "    from pytorch_vqa.model import Net, apply_attention, tile_2d_over_nd  # from pytorch-vqa\n",
    "    from pytorch_vqa.utils import get_transform  # from pytorch-vqa\n",
    "except:\n",
    "    print(\"please provide a valid path to pytorch-vqa\")\n",
    "    \n",
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.attr_vis.features import ImageFeature, TextFeature\n",
    "from captum.attr import TokenReferenceBase, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38caab69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set the device we will use for model inference\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9daa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_state = torch.load(VQA_MODEL_PATH, map_location=device)\n",
    "\n",
    "# reading vocabulary from saved model\n",
    "vocab = saved_state[\"vocab\"]\n",
    "\n",
    "# reading word tokens from saved model\n",
    "token_to_index = vocab[\"question\"]\n",
    "\n",
    "# reading answers from saved model\n",
    "answer_to_index = vocab[\"answer\"]\n",
    "\n",
    "num_tokens = len(token_to_index) + 1\n",
    "\n",
    "# reading answer classes from the vocabulary\n",
    "answer_words = [\"unk\"] * len(answer_to_index)\n",
    "for w, idx in answer_to_index.items():\n",
    "    answer_words[idx] = w\n",
    "\n",
    "vqa_net = torch.nn.DataParallel(Net(num_tokens), device_ids=[0, 1])\n",
    "vqa_net.load_state_dict(saved_state[\"weights\"])\n",
    "vqa_net = vqa_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f140a624",
   "metadata": {},
   "outputs": [],
   "source": [
    " # for visualization to convert indices to tokens for questions\n",
    "question_words = [\"unk\"] * num_tokens\n",
    "for w, idx in token_to_index.items():\n",
    "    question_words[idx] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a80232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537b456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25cbd26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQA environment",
   "language": "python",
   "name": "test_vqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
